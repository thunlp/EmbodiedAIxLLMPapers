# Embodied AI x LLM Papers

This is a paper list on integrating large language models with embodied AI. Large language models have shown sparks of artificial general intelligence, but they are not grounded in the physical world, lacking human-like embodied intelligence. The integration of LLM with embodied AI is undertaken to address this challenge.

### Keywords Convention
<p><img align="center" height="20" src="https://img.shields.io/badge/GATO-52b5f7?style=flat-square"> The abbreviation of the work. </p>
<p><img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square"> The role of LLM in the work. </p> 
<p><img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square"> The mainly explored domains of the work. </p>

## Papers

### Embodied LLM

* **A Generalist Agent.** TMLR 2022.

  *Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas* [[pdf](https://arxiv.org/abs/2205.06175)], 2022.5
  <img align="center" height="20" src="https://img.shields.io/badge/GATO-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game, robot, robot--arm-20b2aa?style=flat-square">
  
* **PaLM-E: An Embodied Multimodal Language Model.** ICML 2023.

  *Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence* [[pdf](https://arxiv.org/abs/2303.03378)], [[page](https://palm-e.github.io/)], 2023.3
  <img align="center" height="20" src="https://img.shields.io/badge/PaLM--E-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot, robot--arm-20b2aa?style=flat-square">

* **EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought.** Preprint.

  *Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo* [[pdf](https://arxiv.org/abs/2305.15021)], 2023.5
  <img align="center" height="20" src="https://img.shields.io/badge/EmbodiedGPT-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot, robot--arm-20b2aa?style=flat-square">
  
* **Language Models Meet World Models: Embodied Experiences Enhance Language Models.** Preprint.

  *Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu* [[pdf](https://arxiv.org/abs/2305.10626)], [[page](https://github.com/szxiangjn/world-model-for-language-model)], 2023.5
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation.** Preprint.

  *Chuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, Jianlong Fu* [[pdf](https://arxiv.org/abs/2305.18898)], [[video](https://www.youtube.com/watch?v=ayAzID1_qQk)], 2023.5
  <img align="center" height="20" src="https://img.shields.io/badge/AlphaBlock-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.** Preprint.

    *Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich* [[pdf](https://arxiv.org/abs/2307.15818)], [[page](https://general-pattern-machines.github.io/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/RT--2-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **Embodied Task Planning with Large Language Models.** Preprint.

    *Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan* [[pdf](https://arxiv.org/abs/2307.01848)], [[page](https://gary3410.github.io/TaPA/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/TaPA-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **Large Language Models as General Pattern Machines.** CoRL 2023.

    *Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng* [[pdf](https://arxiv.org/abs/2307.04721)], [[page](https://robotics-transformer2.github.io/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **Large Language Models as Generalizable Policies for Embodied Tasks.** Preprint.

  *Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev* [[pdf](https://arxiv.org/abs/2310.17722)], [[page](https://llm-rl.github.io/)], 2023.10
  <img align="center" height="20" src="https://img.shields.io/badge/LLaRP-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">

* **Octopus: Embodied Vision-Language Programmer from Environmental Feedback.** Preprint.

  *Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu* [[pdf](https://arxiv.org/abs/2310.08588)], [[page](https://choiszt.github.io/Octopus/)], 2023.10
  <img align="center" height="20" src="https://img.shields.io/badge/Octopus-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game, robot-20b2aa?style=flat-square">

* **Vision-Language Foundation Models as Effective Robot Imitators.** Preprint.

  *Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong* [[pdf](https://arxiv.org/pdf/2311.01378.pdf)], [[page](https://roboflamingo.github.io/)], 2023.11
  <img align="center" height="20" src="https://img.shields.io/badge/RoboFlamingo-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **An Embodied Generalist Agent in 3D World.** Preprint.

  *Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang* [[pdf](https://arxiv.org/abs/2311.12871)], [[page](https://embodied-generalist.github.io/)], 2023.11
  <img align="center" height="20" src="https://img.shields.io/badge/LEO-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/controller-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot, robot--arm-20b2aa?style=flat-square">

<br/>

### LLM for Planning, Tool Using and Beyond (without training)

* **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.** ICML 2022.

  *Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch* [[pdf](https://arxiv.org/abs/2201.07207)], [[page](https://huangwl18.github.io/language-planner)], 2022.1
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">

* **Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.** CoRL 2022.

  *Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng* [[pdf](https://arxiv.org/abs/2201.07207)], [[page](https://say-can.github.io/)], 2022.4
  <img align="center" height="20" src="https://img.shields.io/badge/SayCan-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.** ICLR 2023.

  *Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence* [[pdf](https://arxiv.org/abs/2204.00598)], 2022.4
  <img align="center" height="20" src="https://img.shields.io/badge/SMs-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/tool-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **Code as Policies: Language Model Programs for Embodied Control.** ICRA 2023.

  *Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng* [[pdf](https://arxiv.org/abs/2209.07753)], [[page](https://code-as-policies.github.io/)], 2022.9
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">
  
* **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models.** ICRA 2023.

  *Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg* [[pdf](https://arxiv.org/abs/2209.11302)], [[page](https://progprompt.github.io/)], 2022.9
  <img align="center" height="20" src="https://img.shields.io/badge/ProgPrompt-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot, robot--arm-20b2aa?style=flat-square">

* **LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.** Preprint.

  *Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su* [[pdf](https://arxiv.org/abs/2212.04088)], [[page](https://dki-lab.github.io/LLM-Planner/)], 2022.12
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.** NeurIPS 2023.

  *Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang* [[pdf](https://arxiv.org/abs/2302.01560)], [[page](https://github.com/CraftJarvis/MC-Planner)], 2023.2
  <img align="center" height="20" src="https://img.shields.io/badge/DEPS-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square">

* **Voyager: An Open-Ended Embodied Agent with Large Language Models.** Preprint.

  *Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar* [[pdf](https://arxiv.org/abs/2305.16291)], [[page](https://voyager.minedojo.org/)], 2023.5
  <img align="center" height="20" src="https://img.shields.io/badge/Voyager-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square">

* **Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.** Preprint.

  *Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai* [[pdf](https://arxiv.org/abs/2305.17144)], [[page](https://github.com/OpenGVLab/GITM)], 2023.5
  <img align="center" height="20" src="https://img.shields.io/badge/GITM-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square">
  
* **VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models.** ICML 2022.

  *Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei* [[pdf](https://arxiv.org/abs/2307.05973)], [[page](https://voxposer.github.io/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/VoxPoser-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/tool-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot--arm-20b2aa?style=flat-square">

* **Building Cooperative Embodied Agents Modularly with Large Language Models.** Preprint.

  *Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan* [[pdf](https://arxiv.org/abs/2307.02485)], [[page](https://vis-www.cs.umass.edu/Co-LLM-Agents/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **Building Cooperative Embodied Agents Modularly with Large Language Models.** Preprint.

  *Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan* [[pdf](https://arxiv.org/abs/2307.02485)], [[page](https://vis-www.cs.umass.edu/Co-LLM-Agents/)], 2023.7
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/robot-20b2aa?style=flat-square">
  
* **JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models.** Preprint.

  *Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang* [[pdf](https://arxiv.org/abs/2311.05997)], [[page](https://craftjarvis.org/JARVIS-1)], 2023.11
  <img align="center" height="20" src="https://img.shields.io/badge/JARVIS--1-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/planner-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square">
  
<br/>

### LLM for Supervision

* **Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling.** ICML 2023.

  *Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox* [[pdf](https://arxiv.org/abs/2301.12050)], [[page](https://deckardagent.github.io/)], 2023.1
  <img align="center" height="20" src="https://img.shields.io/badge/DECKARD-52b5f7?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/supervisor-green?style=flat-square">
  <img align="center" height="20" src="https://img.shields.io/badge/game-20b2aa?style=flat-square">
